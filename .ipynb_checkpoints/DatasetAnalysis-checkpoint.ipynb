{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LRS3 Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to get a better understanding of the data format provided in the dataset to make\n",
    "preprocessing choices easier and better.\n",
    "The dataset is available at [the homepage.](https://www.robots.ox.ac.uk/~vgg/data/lip_reading/lrs3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from more_itertools import ilen\n",
    "from collections import defaultdict\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/mnt/U/Datasets/lrs3pretrain/pretrain/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset contains 118516 utterances by 5089 speakers\n"
     ]
    }
   ],
   "source": [
    "speakers = list(data_dir.glob(\"*\"))\n",
    "utt_per_spk = {x.name: ilen(x.glob(\"*.mp4\")) for x in speakers}\n",
    "utterances = sum(utt_per_spk.values())\n",
    "print(f\"The dataset contains {utterances} utterances by {len(speakers)} speakers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowest amount of utterances: 1 by 0e3lCYwtm9o\n",
      "Highest amount of utterances: 105 by KQEWc3LVfyc\n"
     ]
    }
   ],
   "source": [
    "min_spk, min_amount = min(utt_per_spk.items(), key=lambda x: x[1])\n",
    "max_spk, max_amount = max(utt_per_spk.items(), key=lambda x: x[1])\n",
    "print(f\"Lowest amount of utterances: {min_amount} by {min_spk}\")\n",
    "print(f\"Highest amount of utterances: {max_amount} by {max_spk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utterance Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utt_lengths(utt_file):\n",
    "    with utt_file.open() as f:\n",
    "        lines = f.readlines()\n",
    "        words = [x.split(\" \")[0] for x in lines[4:]]\n",
    "        length = float(lines[-1].split(\" \")[2])\n",
    "    return words, length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5089/5089 [00:11<00:00, 450.96it/s]\n"
     ]
    }
   ],
   "source": [
    "all_words = defaultdict(lambda: 0)\n",
    "total_length = 0\n",
    "for speaker in tqdm(speakers):\n",
    "    for text in speaker.glob(\"*.txt\"):\n",
    "        words, length = utt_lengths(text)\n",
    "        for word in words:\n",
    "            all_words[word] += 1\n",
    "        total_length += length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_words = sum(all_words.values())\n",
    "times_spoken = list(sorted(all_words.items(), key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 3888467 words are spoken.\n",
      "Total audio length is 402.98 hours.\n",
      "Average utterance length is 12.24 seconds.\n",
      "Average audio per speaker is 4.75 minutes\n",
      "Each speaker says 764.09 unique words on average.\n",
      "The top ten most spoken words are:\n",
      "  1: THE spoken 166261 times\n",
      "  2: AND spoken 130340 times\n",
      "  3: TO spoken 128146 times\n",
      "  4: OF spoken 96142 times\n",
      "  5: A spoken 83456 times\n",
      "  6: THAT spoken 80989 times\n",
      "  7: I spoken 76177 times\n",
      "  8: IN spoken 67924 times\n",
      "  9: YOU spoken 55460 times\n",
      "  10: WE spoken 50998 times\n"
     ]
    }
   ],
   "source": [
    "print(f\"A total of {amount_words} words are spoken.\")\n",
    "print(f\"Total audio length is {(total_length/3600):.2f} hours.\")\n",
    "print(f\"Average utterance length is {(total_length/utterances):.2f} seconds.\")\n",
    "print(f\"Average audio per speaker is {(total_length/len(speakers)/60):.2f} minutes\")\n",
    "print(f\"Each speaker says {(amount_words/len(speakers)):.2f} unique words on average.\")\n",
    "print(\"The top ten most spoken words are:\")\n",
    "for i, (word, amount) in enumerate(times_spoken[:10]):\n",
    "        print(f\"  {i+1}: {word} spoken {amount} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18674 words are only spoken once.\n",
      "Here are some examples:\n",
      "  REGAINS\n",
      "  VIBRANCE\n",
      "  GEMMA\n",
      "  EUTHANASIA\n",
      "  BARNABY\n",
      "  MIGHTIER\n",
      "  OAU\n",
      "  SEB\n",
      "  HEALTHFUL\n",
      "  ANIMATES\n"
     ]
    }
   ],
   "source": [
    "words_spoken_once = [x for x, y in all_words.items() if y == 1]\n",
    "print(f\"{len(words_spoken_once)} words are only spoken once.\")\n",
    "print(\"Here are some examples:\")\n",
    "for word in words_spoken_once[:10]:\n",
    "    print(f\"  {word}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis\n",
    "This section uses the [Transformers](https://github.com/huggingface/transformers) natural language processing library for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline('sentiment-analysis', device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence(utt_file):\n",
    "    with utt_file.open() as f:\n",
    "        lines = f.readlines()\n",
    "        sentence = lines[0].split(\"Text: \")[1].strip()\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5089/5089 [03:56<00:00, 21.52it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "sentences = {}\n",
    "for i, speaker in enumerate(tqdm(speakers)):\n",
    "    speaker_sentences = []\n",
    "    for text in speaker.glob(\"*.txt\"):\n",
    "        sent = get_sentence(text)\n",
    "        if len(sent) < 1000:\n",
    "            speaker_sentences.append(sent)\n",
    "    sentiments = []\n",
    "    for i in range(math.ceil(len(speaker_sentences)/batch_size)):\n",
    "        sentiments += classifier(speaker_sentences[i*batch_size:(i+1)*batch_size])\n",
    "    for i, sentence in enumerate(speaker_sentences):\n",
    "        sentences[sentence] = sentiments[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "total = 0\n",
    "for sentence, sentiment in sentences.items():\n",
    "    if sentiment[\"label\"] == \"POSITIVE\":\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1\n",
    "    total += sentiment[\"score\"]\n",
    "average = total/utterances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dataset has 57114 positive and 60335 negative utterances.\n",
      "The average confidence is 0.94717\n",
      "1067 utterances were skipped because they were too long.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The dataset has {pos} positive and {neg} negative utterances.\")\n",
    "print(f\"The average confidence is {average:.5f}\")\n",
    "print(f\"{utterances-neg-pos} utterances were skipped because they were too long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most positive sentences are:\n",
      "  COLOR LAUGHTER AND OF COURSE MUSIC TO REMEMBER ME BY THANK YOU --> 0.999893\n",
      "  THIS WONDER OF NATURE FLOURISH AND PROSPER THANK YOU --> 0.999892\n",
      "  GATHER IN LARGE GROUPS THAT ARE AMONG THE MOST WONDERFUL SPECTACLES IN THE NATURAL --> 0.999891\n",
      "  ALONE WE ARE SMART TOGETHER WE ARE BRILLIANT --> 0.999891\n",
      "  SHARE OF OURSELVES TO BE VULNERABLE AND IT'S VERY EXCITING SO THANK YOU --> 0.999891\n",
      "  A MORE LIVABLE AND A MORE DELICIOUS FUTURE THANK YOU --> 0.999891\n",
      "  BY THE RAPIDITY OF THIS LEARNING THEY ARE QUITE RIGHTLY AMAZED AND DELIGHTED BY THEIR CHILDREN'S CLEVERNESS --> 0.999891\n",
      "  AND HE HAS A REALLY WONDERFUL TURN OF PHRASE AND WHAT A SENSE OF VISION THAT HE --> 0.999891\n",
      "  LIKE FASHIONABLY HIP AND REALLY COOL AND GREAT --> 0.999890\n",
      "  FORMS MOST BEAUTIFUL AND MOST WONDERFUL AND I LIKE TO THINK HE --> 0.999890\n"
     ]
    }
   ],
   "source": [
    "most_pos = list(sorted([(t, s) for t, s in sentences.items() if s[\"label\"] == \"POSITIVE\"], key=lambda x: x[1][\"score\"], reverse=True))\n",
    "print(f\"The most positive sentences are:\")\n",
    "for t, s in most_pos[:10]:\n",
    "    print(f\"  {t.strip()} --> {s['score']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most negative sentences are:\n",
      "  WHERE WE'RE GOING YOU KNOW WHY IS IT NOT DONE A LOT OF IT IS JUST COMPUTING'S KIND OF A MESS YOU KNOW YOUR COMPUTER DOESN'T KNOW WHERE YOU ARE IT DOESN'T KNOW WHAT YOU'RE DOING --> 0.999821\n",
      "  CONGESTION WELL AS YOU'VE HEARD CONGESTION IS MAJOR NUISANCE TO ALL OF US LOSS OF TIME MEANS LOSS OF PRODUCTION AND LOSS OF TIME THAT WE COULD USE TO DO MORE FUN THINGS WELFARE --> 0.999821\n",
      "  TO ME IT'S REALLY AN UGLY UNIVERSE IT'S STUPIDLY CONSTRUCTED IT'S GOT WAY TOO MANY ARBITRARY COUPLING CONSTANTS AND MASS RATIOS AND SUPERFLUOUS FAMILIES OF ELEMENTARY PARTICLES AND WHAT THE HELL IS DARK ENERGY IT'S --> 0.999821\n",
      "  IT'LL BE RAINY I'LL GET DEPRESSED THE WHOLE THING WILL BE A HUGE WASTE OF TIME NUMBER TWO I'LL MISS A LETTER FROM THE IRS AND I'LL GET AUDITED OR RAIDED OR SHUT DOWN OR SOME --> 0.999817\n",
      "  DOING A FAILED PROJECT AND SO LIKE I SAID I THINK THE IDEA TO ACTION STRATEGY NEEDS A PROCESS FOR US TO GO THROUGH OTHERWISE WE JUST HAVE THESE RANDOM IDEAS GOING NOWHERE AND --> 0.999816\n",
      "  FAILED TO REALLY TELL YOU JUST HOW ITERATIVE INTERRELATED AND FRANKLY MESSY THEIR PROCESS WAS --> 0.999816\n",
      "  EVEN WORSE THESE POORLY DESIGNED HOUSES ARE --> 0.999815\n",
      "  AND IT BECOMES WORSE WHEN IT'S ENSHRINED IN RELIGIOUS FATWAS BASED ON WRONG INTERPRETATION OF THE SHARIA {NS} {NS} WHAT'S WORST WHEN THEY BECOME CODIFIED AS LAWS IN THE SYSTEM AND WHEN --> 0.999815\n",
      "  TIRED AND IRRITABLE TO HAVE TO MAKE UP ALL OF THAT DATA YOU'RE HAVING TO IMAGINE IT IT'S NOT GOOD FOR YOU IN THE LONG RUN THE THIRD --> 0.999814\n",
      "  THEY FIND THE BODY TOO MESSY TOO UNRELIABLE TO GUARANTEE ETERNAL LIFE AND --> 0.999813\n"
     ]
    }
   ],
   "source": [
    "most_pos = list(sorted([(t, s) for t, s in sentences.items() if s[\"label\"] == \"NEGATIVE\"], key=lambda x: x[1][\"score\"], reverse=True))\n",
    "print(f\"The most negative sentences are:\")\n",
    "for t, s in most_pos[:10]:\n",
    "    print(f\"  {t.strip()} --> {s['score']:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esptest3",
   "language": "python",
   "name": "esptest3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
